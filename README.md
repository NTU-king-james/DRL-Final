# QMIX-LLM: LLM-Guided Multi-Agent Reinforcement Learning

![Version](https://img.shields.io/badge/version-1.0-blue)
![Status](https://img.shields.io/badge/status-completed-success)

A comprehensive framework for evaluating Large Language Model (LLM) guidance in Multi-Agent Reinforcement Learning using QMIX on StarCraft II micromanagement scenarios.

## ğŸ¯ Project Overview

This project implements and evaluates a novel QMIX+LLM collaborative framework where LLM strategic guidance is combined with QMIX reinforcement learning for improved multi-agent coordination. The system includes a full ablation study setup with comprehensive visualization and analysis tools.

### Key Research Questions

1. **Baseline Performance**: How much does LLM guidance improve over pure QMIX?
2. **LLM Quality Impact**: Does fine-tuning improve guidance effectiveness?
3. **Alignment Mechanism**: What's the optimal balance between LLM guidance and environment reward?
4. **Training Stability**: Does LLM guidance improve or hurt QMIX convergence?

## ğŸ› ï¸ Setup & Installation

### Environment Requirements

```bash
# Clone repository
git clone https://github.com/yourusername/qmix-llm.git
cd qmix-llm

# Setup environment
conda create -n pysc2-env python=3.8
conda activate pysc2-env

# Install requirements
pip install -r requirements.txt

# Install StarCraft II dependencies, in pymarl2
bash install_sc2.sh
bash install_dependencies.sh
```

### Map Installation

1. Install PySC2
2. Install SMAC
3. Install SC2
4. Get SMAC maps and put it in SC2PATH/Maps

## ğŸ§ª Experiment Categories

### 1. Baselines

#### Pure QMIX (ç„¡ LLM æŒ‡å°)

```bash
python test_llm.py --llm none --algo qmix --alignment-weight 0.0
```

- **Purpose**: Basic QMIX without LLM guidance
- **Expected**: Standard QMIX performance baseline

#### Pure LLM Execution (ç´” LLM åŸ·è¡Œ)

```bash
# Pre-trained LLM direct execution
python test_llm.py --llm llama3 --algo none

# Fine-tuned LLM direct execution  
python test_llm.py --llm ours --algo none
```

- **Purpose**: LLM as direct policy (expert upper bound reference)
- **Expected**: Shows LLM's raw strategic capability

### 2. LLM Quality & Adaptability Ablations (Fixed alignment_weight=0.5)

#### QMIX + Pre-trained LLM Guidance

```bash
python test_llm.py --llm llama3 --algo qmix --alignment-weight 0.5
```

- **Purpose**: Current implementation with general LLM
- **Expected**: Moderate improvement over pure QMIX

#### QMIX + Fine-tuned LLM Guidance (æ ¸å¿ƒå‰µæ–°é»)

```bash
python test_llm.py --llm ours --algo qmix --alignment-weight 0.5
```

- **Purpose**: **Main experimental group** - domain-specific LLM guidance
- **Expected**: **Best performance** due to specialized knowledge

#### QMIX + Random LLM Guidance

```bash
python test_llm.py --llm random --algo qmix --alignment-weight 0.5
```

- **Purpose**: Control for LLM "intelligence" vs. additional signal
- **Expected**: Minimal improvement, proves LLM quality matters

## ğŸš€ Usage Examples

### Environment Setup

```bash
# IMPORTANT: Always use the pysc2-env environment
cd /root/DRL-Final

# Method 1: If conda activate works
conda activate /root/miniconda3/envs/pysc2-env

# Method 2: Use absolute path (recommended)
/root/miniconda3/envs/pysc2-env/bin/python test_llm.py [arguments]
```

### Individual Experiments

```bash
# Test a specific configuration (using fixed alignment weight 0.5)
python test_llm.py --llm llama3 --algo qmix --alignment-weight 0.5 --episodes 10

# With verbose logging
python test_llm.py --llm ours --algo qmix --alignment-weight 0.5 --verbose --episodes 5

# Different map
python test_llm.py --llm llama3 --algo qmix --alignment-weight 0.5 --map 8m --episodes 10
```

### Full Ablation Study

```bash
# Run all experiments automatically
python run_full_ablation.py

# Results will be saved to ablation_results_TIMESTAMP/
```

## ğŸ“Š Results & Visualization

The project includes a comprehensive visualization and analysis system:

### Analysis Functions

```bash
# Standard analysis
python analyze_results.py

# Comprehensive analysis with plots
python analyze_results.py --comprehensive

# Generate plots only
python analyze_results.py --plots-only

# Analyze specific directory
python analyze_results.py --results-dir /path/to/results --comprehensive
```

### Visualization Outputs

The system generates 4 high-quality research plots:

1. **Baseline Comparison**: QMIX learning curve vs pure LLM performance lines
2. **LLM Quality Comparison**: Multi-curve comparison of different LLM guidance types
3. **Alignment Weight Sensitivity**: Final performance vs alignment weight analysis
4. **Performance Comparison**: Overall performance ranking bar chart

## ğŸ“„ Command Line Arguments

| Argument | Choices | Default | Description |
|----------|---------|---------|-------------|
| `--llm` | `llama3`, `ours`, `random`, `none` | `llama3` | LLM type for guidance |
| `--algo` | `qmix`, `none` | `qmix` | Algorithm choice |
| `--alignment-weight` | float | `0.5` | Weight for alignment reward |
| `--map` | string | `3m` | SMAC map name |
| `--episodes` | int | `10` | Number of episodes |
| `--max-steps` | int | `100` | Max steps per episode |
| `--verbose` | flag | `False` | Enable detailed logging |
| `--render` | flag | `False` | Render environment |
| `--save-replay` | flag | `False` | Save game replays |
| `--seed` | int | `0` | Random seed |

## ğŸ“ Expected Results Hierarchy

1. **QMIX + Fine-tuned LLM** (best performance)
2. **QMIX + Pre-trained LLM** (good performance)  
3. **Pure Fine-tuned LLM** (expert reference)
4. **Pure QMIX** (baseline)
5. **QMIX + Random LLM** (minimal improvement)
6. **Pure Pre-trained LLM** (variable)

## ğŸ”— System Architecture

```
test_llm.py (Main Implementation)
â”œâ”€â”€ LLMAgent (llama3/ours/random/none)
â”œâ”€â”€ QMIXAgent (with alignment reward mechanism)
â”œâ”€â”€ RandomAgent (baseline)
â””â”€â”€ run_smac_with_agents() (collaboration framework)

run_full_ablation.py (Experiment Runner)
â”œâ”€â”€ 10 pre-configured experiments
â”œâ”€â”€ Result saving (JSON + logs)
â””â”€â”€ Error handling & timeouts

analyze_results.py (Analysis & Visualization Tool)
â”œâ”€â”€ Log parsing for metrics
â”œâ”€â”€ Summary generation  
â”œâ”€â”€ Best configuration identification
â”œâ”€â”€ Four core research plots
â”œâ”€â”€ Comprehensive statistical analysis
â”œâ”€â”€ Demo data generation
â””â”€â”€ Multiple analysis modes
```

## ğŸ“Š Latest Visualization Results

The complete ablation study has been visualized with the 170-episode comparison showing clear performance advantages of the QMIX+LLM approach. View the generated plots in the `ablation_results` directory.

## ğŸ“š Notes

- The `ours` LLM type uses the `remijang/smac-sft-gemma3` model
- Adjust the model name in the code if you want to use a different model
- Increase `--episodes` for more robust statistical results (recommended: 50-100)
- Use `--verbose` flag for debugging individual experiments
- Results are saved with timestamps to prevent overwriting

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- PyMARL2 framework for QMIX implementation
- SMAC for multi-agent StarCraft II environment
- Meta AI for Llama 3 models
